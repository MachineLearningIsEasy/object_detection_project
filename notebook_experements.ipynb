{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(src, figsize=(10,10)):\n",
    "    '''\n",
    "    Описание:\n",
    "        Вывод картинки src.\n",
    "    Вход:\n",
    "        src - исходный массив np\n",
    "        figsize - размер выводимой картинки\n",
    "    Выход:\n",
    "        -\n",
    "    ''' \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(src)\n",
    "    plt.show()\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou\n",
    "def get_stop_line_box():\n",
    "    # return (0, 348, 408, 700)\n",
    "    # return (348,0, 372, 408)\n",
    "    return (348,0, 348+372, 408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "path_to_saved_model = \"models\\\\my_model_ssd_mobilenet_v2_fpnlite\\\\1\"\n",
    "path_to_images = 'data\\\\photo'\n",
    "stop_line_box = get_stop_line_box()\n",
    "iou_thresh = 0.1\n",
    "\n",
    "USE_TF_SERVING = False\n",
    "detection_model = None\n",
    "if not USE_TF_SERVING:\n",
    "    detection_model = tf.saved_model.load(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names = ['file503.png', 'file504.png','file529.png','file570.png','file575.png','file586.png']\n",
    "stop_line_box = get_stop_line_box()\n",
    "\n",
    "for image_name in file_names:\n",
    "    # Read image\n",
    "    image_path = os.path.join(path_to_images, image_name)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Requests to models server \n",
    "    image_np = np.array(image)\n",
    "    w_image, h_image, _ = image_np.shape\n",
    "    \n",
    "    if USE_TF_SERVING:\n",
    "        payload = {\"instances\": [image_np.tolist()]}\n",
    "        res = requests.post(\"http://192.168.99.100:8501/v1/models/detection:predict\", json=payload)\n",
    "        detections = res.json()[\"predictions\"][0]\n",
    "    else:\n",
    "        input_tensor = tf.convert_to_tensor(image_np)\n",
    "        input_tensor = input_tensor[tf.newaxis, ...]\n",
    "        detections = detection_model(input_tensor)\n",
    "    \n",
    "    # Prepare response\n",
    "    scores = np.array(detections['detection_scores']).squeeze()\n",
    "    labels = np.array(detections['detection_classes']).squeeze()\n",
    "\n",
    "    boxes = np.array(detections['detection_boxes']).squeeze()\n",
    "    boxes = np.array(boxes)\n",
    "    boxes = boxes*np.array([w_image, h_image, w_image, h_image])\n",
    "    boxes = boxes.astype('int16')\n",
    "\n",
    "    # Visualize results\n",
    "    res_image = image_np.copy()\n",
    "    score_tresh = 0.30\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > score_tresh:\n",
    "            box = boxes[i] \n",
    "            box_color = (0,255,0)\n",
    "            # TODO: ALARM! Update base!\n",
    "            if  bb_intersection_over_union(box, stop_line_box) > iou_thresh:\n",
    "                print(bb_intersection_over_union(box, stop_line_box))\n",
    "                box_color = (255,0,0)\n",
    "            \n",
    "            # print box\n",
    "            cv2.rectangle(res_image, (box[1], box[0]), (box[3], box[2]), box_color)\n",
    "\n",
    "    cv2.rectangle( \n",
    "        res_image, \n",
    "        (stop_line_box[1], stop_line_box[0]), \n",
    "        (stop_line_box[3], stop_line_box[2]), \n",
    "        (255,0,0))\n",
    "    show_img(res_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video with write result video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(src, figsize=(10,10)):\n",
    "    '''\n",
    "    Описание:\n",
    "        Вывод картинки src.\n",
    "    Вход:\n",
    "        src - исходный массив np\n",
    "        figsize - размер выводимой картинки\n",
    "    Выход:\n",
    "        -\n",
    "    ''' \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(src)\n",
    "    plt.show()\n",
    "\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou\n",
    "\t\n",
    "def get_stop_line_box():\n",
    "    return (200, 200, 600, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_video =  'data\\\\video'\n",
    "path_to_saved_model = \"models\\\\my_model_ssd_mobilenet_v2_fpnlite\\\\1\"\n",
    "video_filename = 'KIT-AXL-2020-12-08-09-10-01-581.mp4'\n",
    "video_out_file_path = 'data\\\\video\\\\{}_out.avi'.format(video_filename)\n",
    "\n",
    "tf_serving_url =\"http://192.168.99.100:8501/v1/models/detection:predict\"\n",
    "\n",
    "seconds = 2\n",
    "iou_thresh = 0.00\n",
    "score_tresh = 0.1\n",
    "USE_TF_SERVING = False\n",
    "\n",
    "stop_line_box = get_stop_line_box()\n",
    "\n",
    "detection_model = None\n",
    "if not USE_TF_SERVING:\n",
    "    detection_model = tf.saved_model.load(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] starting video stream...\")\n",
    "vs = cv2.VideoCapture(os.path.join(path_to_video, video_filename))\n",
    "\n",
    "fps = vs.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "multiplier = int(fps * seconds)\n",
    "\n",
    "frame_count = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frameId = 0\n",
    "\n",
    "video_writer = cv2.VideoWriter()\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read frame\n",
    "    frame = vs.read()\n",
    "    frameId = int(round(vs.get(1)))    \n",
    "    # check to see if we have reached the end of the stream\n",
    "    if frameId >= frame_count:\n",
    "        break\n",
    "\n",
    "    frame = frame[1]\n",
    "    if frameId == 1: \n",
    "        video_writer = cv2.VideoWriter(video_out_file_path, fourcc, 1.0, (frame.shape[1],frame.shape[0]))\n",
    "\n",
    "    # frame = imutils.resize(frame, width=460)\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        # # Detect persons\n",
    "        image_np = frame.copy()\n",
    "        w_image, h_image, _ = image_np.shape\n",
    "        \n",
    "        if USE_TF_SERVING:\n",
    "            payload = {\"instances\": [image_np.tolist()]}\n",
    "            res = requests.post(\"http://192.168.99.100:8501/v1/models/detection:predict\", json=payload)\n",
    "            detections = res.json()[\"predictions\"][0]\n",
    "        else:\n",
    "            input_tensor = tf.convert_to_tensor(image_np)\n",
    "            input_tensor = input_tensor[tf.newaxis, ...]\n",
    "            detections = detection_model(input_tensor)\n",
    "        \n",
    "        # Prepare response\n",
    "        scores = np.array(detections['detection_scores']).squeeze()\n",
    "        labels = np.array(detections['detection_classes']).squeeze()\n",
    "\n",
    "        boxes = np.array(detections['detection_boxes']).squeeze()\n",
    "        boxes = np.array(boxes)\n",
    "        boxes = boxes*np.array([w_image, h_image, w_image, h_image])\n",
    "        boxes = boxes.astype('int16')\n",
    "\n",
    "        # Visualize results\n",
    "        res_image = image_np.copy()\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            if scores[i] > score_tresh:\n",
    "                box = boxes[i] \n",
    "                box_color = (0,255,0)\n",
    "                # TODO: ALARM! Update base!\n",
    "                if  bb_intersection_over_union(box, stop_line_box) > iou_thresh:\n",
    "                    # print(bb_intersection_over_union(box, stop_line_box))\n",
    "                    box_color = (0,0,255)\n",
    "                \n",
    "                # print box\n",
    "                cv2.rectangle(res_image, (box[1], box[0]), (box[3], box[2]), box_color)\n",
    "\n",
    "        cv2.rectangle( \n",
    "            res_image, \n",
    "            (stop_line_box[1], stop_line_box[0]), \n",
    "            (stop_line_box[3], stop_line_box[2]), \n",
    "            (0,0,255))\n",
    "\n",
    "        cv2.imshow(\"Frame\", res_image)\n",
    "        video_writer.write(res_image)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    print('\\r{}/{}'.format(frameId,frame_count),end='')\n",
    "\n",
    "# do a bit of cleanup\n",
    "vs.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video with ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    '''\n",
    "    IOU for person detection in alarm box.\n",
    "    '''\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "path_to_video =  'data\\\\video'\n",
    "video_filename = 'KIT-AXL-2020-12-08-09-10-01-581.mp4'\n",
    "\n",
    "# config\n",
    "path_to_saved_model = \"models\\\\my_model_ssd_mobilenet_v2_fpnlite\\\\1\"\n",
    "tf_serving_url =\"http://192.168.99.100:8501/v1/models/detection:predict\"\n",
    "data_base_path = 'db'\n",
    "seconds = 2\n",
    "iou_thresh = 0.00\n",
    "score_tresh = 0.12\n",
    "USE_TF_SERVING = False\n",
    "WRITE_RESULT_VIDEO = True\n",
    "stop_line_box = (0,0,100,100)\n",
    "\n",
    "# Init steps\n",
    "if WRITE_RESULT_VIDEO:\n",
    "    video_out_file_path = os.path.join(path_to_video, '{}_out.avi'.format(video_filename))\n",
    "detection_model = None\n",
    "if not USE_TF_SERVING:\n",
    "    detection_model = tf.saved_model.load(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"[INFO] starting video stream...\")\n",
    "# Open VideoCapture\n",
    "path_to_videofile = os.path.join(path_to_video, video_filename)\n",
    "vs = cv2.VideoCapture(path_to_videofile)\n",
    "# Open VideoWriter\n",
    "if WRITE_RESULT_VIDEO:\n",
    "    # video_writer = cv2.VideoWriter()\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "# Prepare for frames loop info\n",
    "fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "multiplier = int(fps * seconds)\n",
    "frame_count = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frameId = 0\n",
    "\n",
    "alarm_time_on_video = []\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read frame\n",
    "    frame = vs.read()\n",
    "    frameId = int(round(vs.get(1)))    \n",
    "    # check to see if we have reached the end of the stream\n",
    "    if frameId >= frame_count:\n",
    "        break\n",
    "\n",
    "    frame = frame[1]\n",
    "    if frameId == 1: \n",
    "        if WRITE_RESULT_VIDEO:\n",
    "            video_writer = cv2.VideoWriter(video_out_file_path, fourcc, 1.0, (frame.shape[1],frame.shape[0]))\n",
    "        # Get ROI\n",
    "        stop_line_box = cv2.selectROI('Select box', frame, False, False)\n",
    "        stop_line_box = [\n",
    "            stop_line_box[1], \n",
    "            stop_line_box[0],\n",
    "            stop_line_box[3] + stop_line_box[1],\n",
    "            stop_line_box[2] + stop_line_box[0],\n",
    "            ]\n",
    "        cv2.destroyWindow('Select box') \n",
    "        print(stop_line_box)\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        # Detect persons\n",
    "        image_np = frame.copy()\n",
    "        w_image, h_image, _ = image_np.shape\n",
    "        \n",
    "        if USE_TF_SERVING:\n",
    "            payload = {\"instances\": [image_np.tolist()]}\n",
    "            res = requests.post(\"http://192.168.99.100:8501/v1/models/detection:predict\", json=payload)\n",
    "            detections = res.json()[\"predictions\"][0]\n",
    "        else:\n",
    "            input_tensor = tf.convert_to_tensor(image_np)\n",
    "            input_tensor = input_tensor[tf.newaxis, ...]\n",
    "            detections = detection_model(input_tensor)\n",
    "        \n",
    "        # Prepare response\n",
    "        scores = np.array(detections['detection_scores']).squeeze()\n",
    "        labels = np.array(detections['detection_classes']).squeeze()\n",
    "\n",
    "        boxes = np.array(detections['detection_boxes']).squeeze()\n",
    "        boxes = np.array(boxes)\n",
    "        boxes = boxes*np.array([w_image, h_image, w_image, h_image])\n",
    "        boxes = boxes.astype('int16')\n",
    "\n",
    "        # Visualize results\n",
    "        res_image = image_np.copy()\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            if scores[i] > score_tresh:\n",
    "                box = boxes[i] \n",
    "                box_color = (0,255,0)\n",
    "                # TODO: ALARM! Update base!\n",
    "                if  bb_intersection_over_union(box, stop_line_box) > iou_thresh:\n",
    "                    # print(bb_intersection_over_union(box, stop_line_box))\n",
    "                    box_color = (0,0,255)\n",
    "                    alarm_time_on_video.append(str(datetime.timedelta(seconds=int(seconds*frameId/multiplier))))\n",
    "                    print('-->',str(datetime.timedelta(seconds=int(frameId/multiplier))))\n",
    "                \n",
    "                # print box\n",
    "                cv2.rectangle(res_image, (box[1], box[0]), (box[3], box[2]), box_color)\n",
    "\n",
    "        cv2.rectangle( \n",
    "            res_image, \n",
    "            (stop_line_box[1], stop_line_box[0]), \n",
    "            (stop_line_box[3], stop_line_box[2]), \n",
    "            (0,0,255))\n",
    "\n",
    "        cv2.imshow(\"Frame\", res_image)\n",
    "        if WRITE_RESULT_VIDEO:\n",
    "            video_writer.write(res_image)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    print('\\r{}/{}'.format(frameId,frame_count),end='')\n",
    "\n",
    "# Do a bit of cleanup\n",
    "vs.release()\n",
    "if WRITE_RESULT_VIDEO:\n",
    "    video_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Update db\n",
    "result_json = {\n",
    "    'filename':video_filename,\n",
    "    'w_h':[w_image, h_image],\n",
    "    'alarm_box':stop_line_box,\n",
    "    'alarm_time':alarm_time_on_video\n",
    "}\n",
    "\n",
    "result_json_filename = '{}_db.json'.format(video_filename)\n",
    "with open(os.path.join(data_base_path, result_json_filename), 'w') as f:\n",
    "    json.dump(result_json, f)\n",
    "\n",
    "print(\"[INFO] Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1613593495389",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}